{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Facebook Resellers\n",
    "## Machine Learning Classificaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will build a machine learning model that can classify Facebook posts into sellers, no sellers, and fake sellers. In short, because the unprocessed data had very few useful features to begin with, my main focus was on \n",
    "    1. Extracting meta features and interactions between them\n",
    "    2. Quantifying unique characeristics of each description text through latent semantic analysis. \n",
    "    \n",
    "You either run this code on jupyter notebook or the .py version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Data Exploration & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This script was written with Python 3.6.0 on OSX*\n",
    "\n",
    "You will need to install following Python modules, if you don't have them:\n",
    "  * pandas\n",
    "  * numpy\n",
    "  * re\n",
    "  * datetime\n",
    "  * nltk\n",
    "  * sklearn\n",
    "  * xgboost (http://xgboost.readthedocs.io/en/latest/build.html)\n",
    "  \n",
    "You can run individual cells on Jupyter notebook (https://jupyter.readthedocs.io/en/latest/install.html#new-to-python-and-jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 35182 rows and 11 columns\n",
      "Owner types are {nan, 'user', 'page'}\n",
      "Target labels are {'Fake Seller', 'Reseller', 'No Seller'}\n",
      "Percentage of Fake Seller: 26.08%\n",
      "Percentage of Reseller: 27.24%\n",
      "Percentage of No Seller: 46.69%\n",
      "There are 0 rows that are missing profile_picture\n",
      "There are 0 rows that have picture_labels without pictures_url\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from nltk.stem import SnowballStemmer\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Edit the path to your data if necessary\n",
    "data_path = 'Data&Data Classification Challenge - Facebook - Training Set.csv'\n",
    "\n",
    "# load the training data\n",
    "data = pd.read_csv(data_path, delimiter=\"\\t\")\n",
    "\n",
    "print ('The dataset has {} rows and {} columns'\n",
    "       .format(data.shape[0], data.shape[1]))\n",
    "print ('Owner types are {}' .format(set(data['owner_type'])))\n",
    "print ('Target labels are {}'.format(set(data['INDEX New'])))\n",
    "print ('Percentage of Fake Seller: {:.2f}%'\n",
    "       .format(len(data[data['INDEX New'] == 'Fake Seller']) /\n",
    "               data.shape[0] * 100))\n",
    "print ('Percentage of Reseller: {:.2f}%'\n",
    "       .format(len(data[data['INDEX New'] == 'Reseller']) /\n",
    "               data.shape[0] * 100))\n",
    "print ('Percentage of No Seller: {:.2f}%'\n",
    "       .format(len(data[data['INDEX New'] == 'No Seller']) /\n",
    "               data.shape[0] * 100))\n",
    "print('There are {} rows that are missing profile_picture'\n",
    "      .format(len(data[pd.isnull(data.profile_picture)])))\n",
    "print('There are {} rows that have picture_labels without pictures_url'\n",
    "      .format(len(data[pd.isnull(data.pictures_url) &\n",
    "                       pd.notnull(data.picture_labels)])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset contains the following fields:\n",
    "\n",
    "  * description- this is mostly English description of products but quite a few of them are non-English text as well.\n",
    "  * found_keywords\n",
    "\n",
    "  * found_keywords_occurences\n",
    "  * nb_like- number of likes\n",
    "  * nb_share- number of shares\n",
    "  * owner_type- user or page\n",
    "  * pictures_url\n",
    "  * picture_labels\n",
    "  * INDEX NEW- label, this is what we want to predict\n",
    "  * profile_picture- link to profile pictures. All posts have profile picture and links do not contain anything particularly useful. \n",
    "  * published_at- time when the sale was posted on Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>found_keywords</th>\n",
       "      <th>found_keywords_occurrences</th>\n",
       "      <th>nb_like</th>\n",
       "      <th>nb_share</th>\n",
       "      <th>owner_type</th>\n",
       "      <th>pictures_url</th>\n",
       "      <th>picture_labels</th>\n",
       "      <th>INDEX New</th>\n",
       "      <th>profile_picture</th>\n",
       "      <th>published_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Alladin's cave of beautiful designer brands...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>user</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Seller</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t1.0-1/p50x50/...</td>\n",
       "      <td>02/23/16 04:48 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everyone - let me take a minute to clarify som...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>user</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/hphotos-xap1/v/t...</td>\n",
       "      <td>dog</td>\n",
       "      <td>No Seller</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/hprofile-xlt1/v/...</td>\n",
       "      <td>04/02/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHANEL QUILTED BACKPACK SMALL 23X26CM MQH WITH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>user</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/hphotos-xpa1/v/t...</td>\n",
       "      <td>vehicle</td>\n",
       "      <td>Fake Seller</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/hprofile-xfl1/v/...</td>\n",
       "      <td>03/30/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>user</td>\n",
       "      <td>https://scontent-ord1-1.xx.fbcdn.net/hphotos-x...</td>\n",
       "      <td>handbag, handbag, fashion accessory, hood, clo...</td>\n",
       "      <td>Reseller</td>\n",
       "      <td>https://scontent-ord1-1.xx.fbcdn.net/hprofile-...</td>\n",
       "      <td>09/07/2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Longchamp Zip Around Wallet PM / Whatsapp 012-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>user</td>\n",
       "      <td>https://scontent-ord1-1.xx.fbcdn.net/hphotos-x...</td>\n",
       "      <td>electric blue</td>\n",
       "      <td>Reseller</td>\n",
       "      <td>https://scontent-ord1-1.xx.fbcdn.net/hprofile-...</td>\n",
       "      <td>12/05/2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description found_keywords  \\\n",
       "0  An Alladin's cave of beautiful designer brands...            NaN   \n",
       "1  Everyone - let me take a minute to clarify som...            NaN   \n",
       "2  CHANEL QUILTED BACKPACK SMALL 23X26CM MQH WITH...            NaN   \n",
       "3                                                NaN            NaN   \n",
       "4  Longchamp Zip Around Wallet PM / Whatsapp 012-...            NaN   \n",
       "\n",
       "   found_keywords_occurrences  nb_like  nb_share owner_type  \\\n",
       "0                           0        0         0       user   \n",
       "1                           0       29         0       user   \n",
       "2                           0        0         0       user   \n",
       "3                           0       25         0       user   \n",
       "4                           0        1         0       user   \n",
       "\n",
       "                                        pictures_url  \\\n",
       "0                                                NaN   \n",
       "1  https://scontent.xx.fbcdn.net/hphotos-xap1/v/t...   \n",
       "2  https://scontent.xx.fbcdn.net/hphotos-xpa1/v/t...   \n",
       "3  https://scontent-ord1-1.xx.fbcdn.net/hphotos-x...   \n",
       "4  https://scontent-ord1-1.xx.fbcdn.net/hphotos-x...   \n",
       "\n",
       "                                      picture_labels    INDEX New  \\\n",
       "0                                                NaN    No Seller   \n",
       "1                                                dog    No Seller   \n",
       "2                                            vehicle  Fake Seller   \n",
       "3  handbag, handbag, fashion accessory, hood, clo...     Reseller   \n",
       "4                                      electric blue     Reseller   \n",
       "\n",
       "                                     profile_picture       published_at  \n",
       "0  https://scontent.xx.fbcdn.net/v/t1.0-1/p50x50/...  02/23/16 04:48 AM  \n",
       "1  https://scontent.xx.fbcdn.net/hprofile-xlt1/v/...         04/02/2016  \n",
       "2  https://scontent.xx.fbcdn.net/hprofile-xfl1/v/...         03/30/2016  \n",
       "3  https://scontent-ord1-1.xx.fbcdn.net/hprofile-...         09/07/2015  \n",
       "4  https://scontent-ord1-1.xx.fbcdn.net/hprofile-...         12/05/2015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.head(n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separate target label from feature data which the model will be trained on\n",
    "label = data['INDEX New']\n",
    "features = data.drop('INDEX New', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of fields in the original dataset do not provide much insight. The code below extracts some meta features from these fields so that our model can better capture the characteristics of data.\n",
    "\n",
    "  * `published_hour` (int): at what time of the day was the post created?\n",
    "  * `description_length` (int): word count of `description`\n",
    "  * `picture_label_occurences` (int): how many picture labels does the post have?\n",
    "  * `hashtags` (int): how many hashtags does the post have?\n",
    "  * `punctuations` (int): how many exclamations mark does the post have?\n",
    "  * `has_contact` (int 1 if True, 0 otherwise): did the author of the post mention any personal contact?\n",
    "  * `uppercase_count` (int): how many uppercase characters are in `description`?\n",
    "  * `uppercase_ratio` (float): ratio between uppercase_count and character count of `description`\n",
    "  * `has_pic_url` (int 1 if True, 0 otherwise): does the post have a `picture_url`?\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features['published_hour'] = features.published_at.apply(\n",
    "    lambda x: datetime.strptime(x, '%m/%d/%y %I:%M %p').hour\n",
    "    if len(x) > 10 else np.nan\n",
    ")\n",
    "\n",
    "features['description_length'] = features.description.apply(\n",
    "    lambda x: len(x.split()) if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "# add 1 to differentiate NaN from having no lables\n",
    "features['picture_label_occurrences'] = features.picture_labels.apply(\n",
    "    lambda x: x.count(',') + 1 if (not isinstance(x, float)) or\n",
    "                                  (isinstance(x, float) and not np.isnan(x))\n",
    "    else 0\n",
    ")\n",
    "\n",
    "features['hashtags'] = features.description.apply(\n",
    "    lambda x: x.count('#') if (not isinstance(x, float)) or\n",
    "                              (isinstance(x, float) and not np.isnan(x))\n",
    "    else 0\n",
    ")\n",
    "\n",
    "features['punctuations'] = features.description.apply(\n",
    "    lambda x: x.count('!') if (not isinstance(x, float)) or\n",
    "                              (isinstance(x, float) and not np.isnan(x))\n",
    "    else 0\n",
    ")\n",
    "\n",
    "phone = re.compile(r'\\s[[0-9]{9,10}|[0-9]{3,4}\\s[0-9]{6,7}]\\s')\n",
    "contact_flag = ['call', 'contact', '@', 'whatsapp',\n",
    "                'text', 'message', 'pm', phone]\n",
    "features['has_contact'] = features.description.apply(\n",
    "    lambda x: int(any(bool(re.search(s, re.sub(r'[\\-|\\+|\\(|\\)|\\.|\\,]',\n",
    "                                               '', x.lower())))\n",
    "                      for s in contact_flag))\n",
    "    if (not isinstance(x, float)) or (isinstance(x, float) and not np.isnan(x))\n",
    "    else 0\n",
    ")\n",
    "\n",
    "# add 1 to differentiate NaN from having no uppercase letters\n",
    "features['uppercase_count'] = features.description.apply(\n",
    "    lambda x: (sum(1 for c in x if c.isupper())) + 1\n",
    "    if not isinstance(x, float)\n",
    "    else 0\n",
    ")\n",
    "\n",
    "# add 1 to differentiate NaN from having no uppercase letters\n",
    "features['uppercase_ratio'] = features.description.apply(\n",
    "    lambda x: (sum(1 for c in x if c.isupper()) + 1) / len(x)\n",
    "    if not isinstance(x, float)\n",
    "    else 0\n",
    ")\n",
    "\n",
    "features['has_pic_url'] = features.pictures_url.apply(\n",
    "    lambda x: 0 if not isinstance(x, float) else 1\n",
    ")\n",
    "\n",
    "# remove profile_picture, pictures_url, and\n",
    "# published_at since they are not informative\n",
    "features = features.drop('profile_picture', axis=1) \\\n",
    "    .drop('pictures_url', axis=1) \\\n",
    "    .drop('published_at', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how skewed the distribution of numeric features are. Originally, I had performed logarithmic transformation to change the shape of distribution and make the model more robust to extreme values. However, the model turned out to perform a lot better when you don't do transformation. This is probably due to the fact that having an extreme number of likes and shares, for example, is a rather normal phenomenon in Facebook, particularly for popular posts or official pages. Therefore, although they are outliers, this extremity can be quite informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>found_keywords_occurrences</th>\n",
       "      <th>nb_like</th>\n",
       "      <th>nb_share</th>\n",
       "      <th>published_hour</th>\n",
       "      <th>description_length</th>\n",
       "      <th>picture_label_occurrences</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>has_contact</th>\n",
       "      <th>uppercase_count</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>has_pic_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35182.000000</td>\n",
       "      <td>35182.00000</td>\n",
       "      <td>3.518200e+04</td>\n",
       "      <td>34015.000000</td>\n",
       "      <td>35182.000000</td>\n",
       "      <td>35182.000000</td>\n",
       "      <td>35182.00000</td>\n",
       "      <td>35182.000000</td>\n",
       "      <td>35182.000000</td>\n",
       "      <td>35182.000000</td>\n",
       "      <td>35182.000000</td>\n",
       "      <td>35182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.301063</td>\n",
       "      <td>617.11233</td>\n",
       "      <td>4.375942e+02</td>\n",
       "      <td>10.903307</td>\n",
       "      <td>30.385339</td>\n",
       "      <td>1.371667</td>\n",
       "      <td>0.36135</td>\n",
       "      <td>0.685748</td>\n",
       "      <td>0.193195</td>\n",
       "      <td>20.409556</td>\n",
       "      <td>0.129761</td>\n",
       "      <td>0.217981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.443882</td>\n",
       "      <td>9420.10791</td>\n",
       "      <td>1.289164e+04</td>\n",
       "      <td>6.944151</td>\n",
       "      <td>99.697575</td>\n",
       "      <td>5.586411</td>\n",
       "      <td>3.35685</td>\n",
       "      <td>2.566528</td>\n",
       "      <td>0.394811</td>\n",
       "      <td>56.386471</td>\n",
       "      <td>0.180818</td>\n",
       "      <td>0.412880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.028855</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.072993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>94.000000</td>\n",
       "      <td>672593.00000</td>\n",
       "      <td>1.444213e+06</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>5414.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>245.00000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2972.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       found_keywords_occurrences       nb_like      nb_share  published_hour  \\\n",
       "count                35182.000000   35182.00000  3.518200e+04    34015.000000   \n",
       "mean                     0.301063     617.11233  4.375942e+02       10.903307   \n",
       "std                      1.443882    9420.10791  1.289164e+04        6.944151   \n",
       "min                      0.000000       0.00000  0.000000e+00        0.000000   \n",
       "25%                      0.000000       0.00000  0.000000e+00        5.000000   \n",
       "50%                      0.000000       0.00000  0.000000e+00       10.000000   \n",
       "75%                      0.000000       4.00000  0.000000e+00       17.000000   \n",
       "max                     94.000000  672593.00000  1.444213e+06       23.000000   \n",
       "\n",
       "       description_length  picture_label_occurrences     hashtags  \\\n",
       "count        35182.000000               35182.000000  35182.00000   \n",
       "mean            30.385339                   1.371667      0.36135   \n",
       "std             99.697575                   5.586411      3.35685   \n",
       "min              0.000000                   0.000000      0.00000   \n",
       "25%              3.000000                   0.000000      0.00000   \n",
       "50%             10.000000                   0.000000      0.00000   \n",
       "75%             24.000000                   1.000000      0.00000   \n",
       "max           5414.000000                  89.000000    245.00000   \n",
       "\n",
       "       punctuations   has_contact  uppercase_count  uppercase_ratio  \\\n",
       "count  35182.000000  35182.000000     35182.000000     35182.000000   \n",
       "mean       0.685748      0.193195        20.409556         0.129761   \n",
       "std        2.566528      0.394811        56.386471         0.180818   \n",
       "min        0.000000      0.000000         0.000000         0.000000   \n",
       "25%        0.000000      0.000000         2.000000         0.028855   \n",
       "50%        0.000000      0.000000         6.000000         0.072993   \n",
       "75%        0.000000      0.000000        16.000000         0.148148   \n",
       "max       77.000000      1.000000      2972.000000         2.000000   \n",
       "\n",
       "        has_pic_url  \n",
       "count  35182.000000  \n",
       "mean       0.217981  \n",
       "std        0.412880  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to fill in as many NaN values as possible. Therefore, I replaced NaNs in `published_hour` with the most frequent value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# impute nan values in published_hour\n",
    "hour_imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=1)\n",
    "hour_imp.fit(features.published_hour.values.reshape(1, -1))\n",
    "features.published_hour = sum(hour_imp.transform(\n",
    "    features.published_hour.values.reshape(1, -1)).tolist(), [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, I encoded all categorial variables. The column of `owner_type` is now split into two columns, `user` and `page`, each containing 0s and 1s to indicate which owner type a post has. Data label is also encoded into numbers.\n",
    "\n",
    "Now the labels are\n",
    "\n",
    "  * `Fake Seller`: 0\n",
    "  * `No Seller`: 1\n",
    "  * `Re Seller`: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one hot encode categorial variable\n",
    "encoded_owner = pd.get_dummies(features.owner_type)\n",
    "features = pd.concat([features, encoded_owner], axis=1)\n",
    "\n",
    "# drop uninformative features\n",
    "features = features.drop('owner_type', axis=1) \\\n",
    "    .drop('found_keywords', axis=1) \\\n",
    "    .drop('picture_labels', axis=1)\n",
    "\n",
    "# change label categories from string to integer\n",
    "LE = LabelEncoder()\n",
    "label = LE.fit_transform(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now here is how the feature data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>found_keywords_occurrences</th>\n",
       "      <th>nb_like</th>\n",
       "      <th>nb_share</th>\n",
       "      <th>published_hour</th>\n",
       "      <th>description_length</th>\n",
       "      <th>picture_label_occurrences</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>has_contact</th>\n",
       "      <th>uppercase_count</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>has_pic_url</th>\n",
       "      <th>page</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Alladin's cave of beautiful designer brands...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.037915</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everyone - let me take a minute to clarify som...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.032206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHANEL QUILTED BACKPACK SMALL 23X26CM MQH WITH...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Longchamp Zip Around Wallet PM / Whatsapp 012-...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  An Alladin's cave of beautiful designer brands...   \n",
       "1  Everyone - let me take a minute to clarify som...   \n",
       "2  CHANEL QUILTED BACKPACK SMALL 23X26CM MQH WITH...   \n",
       "3                                                NaN   \n",
       "4  Longchamp Zip Around Wallet PM / Whatsapp 012-...   \n",
       "\n",
       "   found_keywords_occurrences  nb_like  nb_share  published_hour  \\\n",
       "0                           0        0         0             4.0   \n",
       "1                           0       29         0             6.0   \n",
       "2                           0        0         0             6.0   \n",
       "3                           0       25         0             6.0   \n",
       "4                           0        1         0             6.0   \n",
       "\n",
       "   description_length  picture_label_occurrences  hashtags  punctuations  \\\n",
       "0                  33                          0         0             1   \n",
       "1                 111                          1         0             0   \n",
       "2                   9                          1         0             0   \n",
       "3                   0                          5         0             0   \n",
       "4                   8                          1         0             0   \n",
       "\n",
       "   has_contact  uppercase_count  uppercase_ratio  has_pic_url  page  user  \n",
       "0            0                8         0.037915            1     0     1  \n",
       "1            0               20         0.032206            0     0     1  \n",
       "2            0               40         0.714286            0     0     1  \n",
       "3            0                0         0.000000            0     0     1  \n",
       "4            1                8         0.150943            0     0     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(features.head(n=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Natural Language Processing: Extracting Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, I will illustrate how I extracted useful features from the `description` field.\n",
    "\n",
    "For the purpose of demonstration, I randomly split the data into training (80%) and test (20%) datasets. This is not how I actually validated my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 28145 samples.\n",
      "Testing set has 7037 samples.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, label, test_size=0.2, random_state=0)\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I from the traing set, I created a `TfidfVectorizer` object to extract tf-idf (term frequencyâ€“inverse document frequency) to qunatify the importance of each word in the description field. In short, this feature numerically represents how often a given word appears in a description, but downsize its weight by how often it appears across the entire description data. Therefore, uninformative words that appear in a lot of descriptions will have a relatively low number. \n",
    "\n",
    "After fitting `TfidfVectorizer` on the training description text, I applied the same object to description in test data, extracting tf-idf of words that appear in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    processed = re.sub(r'[#|\\!|\\-|\\+|:|//|\\']', \"\", text)\n",
    "    processed = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', ' ', processed).strip()\n",
    "    processed = re.sub('[\\s]+', ' ', processed).strip()\n",
    "    processed = \" \".join([SnowballStemmer(\"english\").stem(word)\n",
    "                          for word in processed.split()])\n",
    "    return processed\n",
    "\n",
    "\n",
    "description_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                         ngram_range=(1, 2),\n",
    "                                         preprocessor=preprocessor)\n",
    "# fit on description in the training set\n",
    "description_vectorizer.fit(X_train.description.values.astype('U'))\n",
    "tfidf_train = description_vectorizer.transform(\n",
    "    X_train.description.values.astype('U'))\n",
    "\n",
    "# extract tf-idf from the test set\n",
    "tfidf_test = description_vectorizer.transform(\n",
    "    X_test.description.values.astype('U'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparse document matrix has 343080 tfi-df features\n"
     ]
    }
   ],
   "source": [
    "print (\"The sparse document matrix has {} tfi-df features\"\n",
    "       .format(tfidf_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are way too many features, which will make the model overfit to training data in the end, I attempted to extract _actually_ informative features. To do so,  I implemented Latent Semantic Analysis (LSA), by applying Singular Value Decomposition (SVD) to reduce the dimension of the matrix. The resultant tfi-df matrix now has 150 components.\n",
    "\n",
    "In hindsight, this model could have had a lot cleaner data if I could deal with foreign text better. Although most posts are in English there are quite a bit of non-English text as well. The ideal thing to do would have been translating these foreign text to English before tf-idf analysis so that we can reduce the number of unique words in the data and capture significant words in non-English posts. I tried to find free API to do this, but because Google has stopped giving free access to its API, most python modules have died out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dimension reduction: LSA\n",
    "tfidf_lsa = TruncatedSVD(n_components=150)\n",
    "# fit TruncatedSVD on the training data.\n",
    "reduced_tfidf_train = tfidf_lsa.fit_transform(tfidf_train)\n",
    "# reduce dimension of the test data\n",
    "reduced_tfidf_test = tfidf_lsa.transform(tfidf_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to merge the features from numerical variables with text features. I also scaled the data so that number ranges wouldn't be too different across different features. \n",
    "\n",
    "Originally, I also tried PCA on non-text features, but the model ended up having much better results without PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no need to keep description anymore. Remove!\n",
    "decomposed_X_train = X_train.drop('description', axis=1)\n",
    "decomposed_X_test = X_test.drop('description', axis=1)\n",
    "\n",
    "# combine text features with the rest.\n",
    "feature_train = np.hstack((decomposed_X_train, reduced_tfidf_train))\n",
    "feature_test = np.hstack((decomposed_X_test, reduced_tfidf_test))\n",
    "\n",
    "# initialize a normalizer, then apply it to the features\n",
    "feature_scaler = MinMaxScaler()\n",
    "feature_scaler.fit(feature_train)\n",
    "feature_train = feature_scaler.transform(feature_train)\n",
    "feature_test = feature_scaler.transform(feature_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for modeling! I put the entire section of text feature extraction into a function so that I can easily loop through different folds during the validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    processed = re.sub(r'[#|\\!|\\-|\\+|:|//|\\']', \"\", text)\n",
    "    processed = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', ' ', processed).strip()\n",
    "    processed = re.sub('[\\s]+', ' ', processed).strip()\n",
    "    processed = \" \".join([SnowballStemmer(\"english\").stem(word)\n",
    "                          for word in processed.split()])\n",
    "    return processed\n",
    "\n",
    "\n",
    "def extract_feature(train_idx, test_idx, feature_df):\n",
    "    X_train = feature_df.iloc[train_idx]\n",
    "    X_test = feature_df.iloc[test_idx]\n",
    "\n",
    "    # tf-idf\n",
    "    description_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                             ngram_range=(1, 2),\n",
    "                                             preprocessor=preprocessor)\n",
    "    description_vectorizer.fit(X_train.description.values.astype('U'))\n",
    "    tfidf_train = description_vectorizer.transform(\n",
    "        X_train.description.values.astype('U'))\n",
    "    tfidf_test = description_vectorizer.transform(\n",
    "        X_test.description.values.astype('U'))\n",
    "\n",
    "    # LSA\n",
    "    tfidf_lsa = TruncatedSVD(n_components=150, random_state=0)\n",
    "    reduced_tfidf_train = tfidf_lsa.fit_transform(tfidf_train)\n",
    "    reduced_tfidf_test = tfidf_lsa.transform(tfidf_test)\n",
    "\n",
    "    # combine text and non-text features\n",
    "    feature_train = np.hstack((X_train.drop('description', axis=1),\n",
    "                              reduced_tfidf_train))\n",
    "    feature_test = np.hstack((X_test.drop('description', axis=1),\n",
    "                             reduced_tfidf_test))\n",
    "\n",
    "    # scale features\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    feature_scaler.fit(feature_train)\n",
    "    feature_train = feature_scaler.transform(feature_train)\n",
    "    feature_test = feature_scaler.transform(feature_test)\n",
    "\n",
    "    return feature_train, feature_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation, I used K-Fold Validation with 5 folds. During the testing process, I noticed that it is absolutely important that I _shuffle_ the data before splitting it into folds. It turned out that there are multiple posts with the same description or very similar advertisement. If you don't shuffle the data at all, the training and test data will each take the clusters of very similar posts and end up having a skewed representation of data about particular brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validate_model(clf_model, features, label):\n",
    "    \"\"\"\n",
    "        clf_model- model object\n",
    "        feaures- original feature dataset\n",
    "        label- original label dataset\n",
    "    \"\"\"\n",
    "    model_result = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    i = 0\n",
    "    for train, test in kf.split(features.values):\n",
    "        i = i + 1\n",
    "        print ('Fold {}'.format(i))\n",
    "        train_final, test_final = extract_feature(train, test, features)\n",
    "        clf_model.fit(train_final, label[train])\n",
    "        output = clf_model.score(test_final, label[test])\n",
    "        model_result.append(output)\n",
    "        # accuracy of individual folds\n",
    "        print ('Accuracy:{}'.format(output))\n",
    "\n",
    "    print ('Overall Accuracy: {}'.format(np.mean(model_result)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried several different classifiers: Decision Tree, Random Forest, Support Vector Machine, Multi-layer Perceptron, Random Forest, AdaBoost, Gradient Boosting, and eXtreme Gradient Boosting. After many rounds of tweaking different parameters, I found the following classifiers to be most accuracte:\n",
    "\n",
    "  * Random Forest Classifier\n",
    "      > because the training data has quite a bit of features (165 different features) and the model has a high risk of overfitting on particular text features, I used 100 trees (estimators) with no depth. In selecting the maximum number of features at each split, I followed the rule of thumb and took the square root of the number of input fearture.\n",
    "      \n",
    "  * Gradient Boosting with Decision Tree Regressor\n",
    "      > I picked 0.1 as learning rate so that the model can generalize better. In return, I also used 150 boosting estimators (trees) to model all possible relation at such a low learning rate. I  limited maximum depth to 10 to prevent overfitting as well. Finally, subsamping is set at 0.8 to reduce variance. \n",
    "      \n",
    "  * AdaBoost with Random Forest as a base estimator\n",
    "      > Instead of using decision tree, I used random forest classifier as a base learner because it seemed to generalize much better than the former. But because it was so computationally expensive, I set max_depth at 5 (as opposed to none above). I wanted to set learning rate low so ended up using 300 estimators.\n",
    "      \n",
    "  * XGBoost\n",
    "      > Like above algorithms, I set learning rate at 0.1 to prevent overfitting and number of boosting rounds at 300 to compensate for the low learning rate. I subsampled 80% of data, also to prevent overfitting. Objective is tuned to be multi:softmax to allow non-binary classification.\n",
    "\n",
    "They all reached similar accuracies, but among them,  XGBoost seemed to be the most practical one. Not only is it slightly more accurate than the other 3, but XGBoost model is also much more efficient than Adaboost and Gradient Boosting. AdaBoost and Gradient Boost take ~30 minutes to train the model (thus taking around 1.5h to test all 5 folds), and therefore all things considered, XGBoost had the best performance overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy:0.8557623987494671\n",
      "Fold 2\n",
      "Accuracy:0.8468097200511582\n",
      "Fold 3\n",
      "Accuracy:0.8473564525298465\n",
      "Fold 4\n",
      "Accuracy:0.8480670835702103\n",
      "Fold 5\n",
      "Accuracy:0.8521887436043206\n",
      "Overall Accuracy: 0.8500368797010005\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None,\n",
    "                             min_samples_split=2, random_state=0)\n",
    "validate_model(clf, features, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy:0.8577518829046469\n",
      "Fold 2\n",
      "Accuracy:0.8584624129600682\n",
      "Fold 3\n",
      "Accuracy:0.8590108015918135\n",
      "Fold 4\n",
      "Accuracy:0.8563104036384309\n",
      "Fold 5\n",
      "Accuracy:0.8524729960204662\n",
      "Overall Accuracy: 0.8568016994230853\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1,\n",
    "                                 subsample=0.8, max_depth=10,\n",
    "                                 max_features='auto', random_state=0)\n",
    "validate_model(clf, features, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy:0.8448202358959784\n",
      "Fold 2\n",
      "Accuracy:0.834588603097911\n",
      "Fold 3\n",
      "Accuracy:0.8344229675952246\n",
      "Fold 4\n",
      "Accuracy:0.8374076179647527\n",
      "Fold 5\n",
      "Accuracy:0.8352757248436612\n",
      "Overall Accuracy: 0.8373030298795057\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=5),\n",
    "                         n_estimators=300, learning_rate=0.1, random_state=0)\n",
    "validate_model(clf, features, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Accuracy:0.8621571692482592\n",
      "Fold 2\n",
      "Accuracy:0.8613045331817536\n",
      "Fold 3\n",
      "Accuracy:0.8560261512222854\n",
      "Fold 4\n",
      "Accuracy:0.8615690733371234\n",
      "Fold 5\n",
      "Accuracy:0.8598635588402501\n",
      "Overall Accuracy: 0.8601840971659342\n"
     ]
    }
   ],
   "source": [
    "# list to store results\n",
    "model_result = []\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# create validatoin set\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "i = 0\n",
    "for train, test in kf.split(features.values):\n",
    "    i = i + 1\n",
    "    print ('Fold {}'.format(i))\n",
    "    train_final, test_final = extract_feature(train, test, features)\n",
    "    \n",
    "    # collect true labels from each fold\n",
    "    actuals = actuals + label[test].tolist()\n",
    "\n",
    "    # prepare the dataset for xgboost\n",
    "    dtrain = xgb.DMatrix(train_final, label=label[train])\n",
    "    dtest = xgb.DMatrix(test_final, label=label[test])\n",
    "    \n",
    "    # parameter setting\n",
    "    param = {'max_depth': 7, 'eta': 0.1, 'objective': 'multi:softmax',\n",
    "             'num_class': 3, 'subsample': 0.8, 'seed': 0}\n",
    "    num_round = 300\n",
    "    clf_xgb = xgb.train(param, dtrain, num_round)\n",
    "    \n",
    "    # make prediction and store it to predictions list\n",
    "    preds = clf_xgb.predict(dtest)\n",
    "    predictions = predictions + preds.tolist()\n",
    "\n",
    "    # accuracy score\n",
    "    output = metrics.accuracy_score(label[test], preds)\n",
    "    model_result.append(output)\n",
    "    # accuracy of individual folds\n",
    "    print ('Accuracy:{}'.format(output))\n",
    "\n",
    "print ('Overall Accuracy: {}'.format(np.mean(model_result)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "Fake Seller       0.87      0.81      0.84      9174\n",
      "  No Seller       0.87      0.90      0.89     16425\n",
      "   Reseller       0.83      0.84      0.83      9583\n",
      "\n",
      "avg / total       0.86      0.86      0.86     35182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(actuals, predictions, target_names = LE.classes_.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
