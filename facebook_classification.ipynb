{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 35182 rows and 11 columns\n",
      "Owner types are {nan, 'user', 'page'}\n",
      "Target labels are {'No Seller', 'Reseller', 'Fake Seller'}\n",
      "Percentage of Fake Seller: 26.08%\n",
      "Percentage of Reseller: 27.24%\n",
      "Percentage of No Seller: 46.69%\n"
     ]
    }
   ],
   "source": [
    "# python version 3.6.0\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "\n",
    "# change working directory\n",
    "data_path = '/Users/jaeyoonjung/Desktop/Data&Data Classification Challenge - Facebook/\\\n",
    "Data&Data Classification Challenge - Facebook - Training Set.csv'\n",
    "\n",
    "# load the training data\n",
    "data = pd.read_csv(data_path, delimiter=\"\\t\")\n",
    "\n",
    "print ('The dataset has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n",
    "print ('Owner types are {}' .format(set(data['owner_type'])))\n",
    "print ('Target labels are {}'.format(set(data['INDEX New'])))\n",
    "print ('Percentage of Fake Seller: {:.2f}%'.format(len(data[data['INDEX New'] == 'Fake Seller']) / \\\n",
    "                                                   data.shape[0] * 100))\n",
    "print ('Percentage of Reseller: {:.2f}%'.format(len(data[data['INDEX New'] == 'Reseller']) / \\\n",
    "                                                   data.shape[0] * 100))\n",
    "print ('Percentage of No Seller: {:.2f}%'.format(len(data[data['INDEX New'] == 'No Seller']) / \\\n",
    "                                                   data.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 rows that are missing profile_picture\n",
      "There are 0 rows that have picture_labels without pictures_url\n"
     ]
    }
   ],
   "source": [
    "label = data['INDEX New']\n",
    "features = data.drop('INDEX New', axis = 1)\n",
    "\n",
    "print('There are {} rows that are missing profile_picture' \\\n",
    "     .format(len(features[pd.isnull(features.profile_picture)])))\n",
    "print('There are {} rows that have picture_labels without pictures_url' \\\n",
    "      .format(len(features[pd.isnull(features.pictures_url) & pd.notnull(features.picture_labels)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features['published_hour'] = features.published_at.apply(\n",
    "    lambda x: datetime.strptime(x, '%m/%d/%y %I:%M %p').hour if len(x) > 10 else np.nan\n",
    ")\n",
    "\n",
    "# features['published_day_of_week'] = features.published_at.apply(\n",
    "#     lambda x: datetime.strptime(x, '%m/%d/%y %I:%M %p').isoweekday() if len(x) > 10 else \\\n",
    "#     datetime.strptime(x, '%m/%d/%Y').isoweekday()\n",
    "# )\n",
    "\n",
    "# word count of description as a new feature\n",
    "features['description_length'] = features.description.apply(\n",
    "    lambda x: len(x.split()) if isinstance(x, str) else 0\n",
    ")\n",
    "\n",
    "# number of picture labels\n",
    "features['picture_label_occurrences'] = features.picture_labels.apply(\n",
    "    lambda x: x.count(',') + 1 if (not isinstance(x, float)) or (isinstance(x, float) and not np.isnan(x))\n",
    "    else 0\n",
    ")\n",
    "\n",
    "# does the post have any hastag?\n",
    "features['hashtags'] = features.description.apply(\n",
    "    lambda x: x.count('#') if (not isinstance(x, float)) or (isinstance(x, float) and not np.isnan(x))\n",
    "    else 0\n",
    ")\n",
    "\n",
    "features['punctuations'] = features.description.apply(\n",
    "    lambda x: x.count('!') if (not isinstance(x, float)) or (isinstance(x, float) and not np.isnan(x))\n",
    "    else 0\n",
    ")\n",
    "\n",
    "# did the writer leave personal contact?\n",
    "phone = re.compile(r'\\s[[0-9]{9,10}|[0-9]{3,4}\\s[0-9]{6,7}]\\s')\n",
    "contact_flag = ['call', 'contact', '@', 'whatsapp', 'text', 'message','pm', phone]\n",
    "\n",
    "features['has_contact'] = features.description.apply(\n",
    "    lambda x: int(any(bool(re.search(s, re.sub(r'[\\-|\\+|\\(|\\)|\\.|\\,]', '',x.lower()))) for s in contact_flag)) \\\n",
    "    if (not isinstance(x, float)) or (isinstance(x, float) and not np.isnan(x)) \\\n",
    "    else 0 \n",
    ")\n",
    "\n",
    "features['uppercase_count'] =  features.description.apply(\n",
    "    lambda x: (sum(1 for c in x if c.isupper())) + 1 if not isinstance(x, float)\n",
    "    else 0\n",
    ")\n",
    "\n",
    "#add one to differentiate NaN\n",
    "features['uppercase_ratio'] =  features.description.apply(\n",
    "    lambda x: (sum(1 for c in x if c.isupper()) + 1) / len(x) if not isinstance(x, float)\n",
    "    else 0\n",
    ")\n",
    "\n",
    "\n",
    "features['has_pic_url'] = features.pictures_url.apply(\n",
    "    lambda x: 0 if not isinstance(x, float) else 1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = features.drop('profile_picture', axis = 1) \\\n",
    "    .drop('pictures_url', axis = 1) \\\n",
    "    .drop('published_at', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distribution of numeric features seems very skewed (except for published_hour and published_day_of_week)\n",
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#impute nan values in published_hour\n",
    "hour_imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=1)\n",
    "hour_imp.fit(features.published_hour.values.reshape(1, -1))\n",
    "features.published_hour = sum(hour_imp.transform(features.published_hour.values.reshape(1, -1)).tolist(),[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one hot encode categorial variable\n",
    "encoded_owner = pd.get_dummies(features.owner_type)\n",
    "features = pd.concat([features, encoded_owner], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = features.drop('owner_type', axis= 1) \\\n",
    "    .drop('found_keywords', axis = 1) \\\n",
    "    .drop('picture_labels', axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LE = LabelEncoder()\n",
    "label = LE.fit_transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fake Seller', 'No Seller', 'Reseller'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LE.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split data for cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#strip punctuation and numbers and extract stem of each feature \n",
    "def preprocessor(text):\n",
    "    #strip punc\n",
    "    processed = re.sub(r'[#|\\!|\\-|\\+|:|//|\\']', \"\", text)\n",
    "    #strip number\n",
    "    processed = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', ' ', processed).strip()\n",
    "    #consolidate whitespace\n",
    "    processed = re.sub('[\\s]+', ' ', processed).strip()\n",
    "    processed =  \" \".join([SnowballStemmer(\"english\").stem(word) for word in processed.split()])\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# pic_label_count = CountVectorizer()\n",
    "# pic_label_count.fit(X_train.picture_labels.values.astype('U'))\n",
    "# pic_label_train = pic_label_count.transform(X_train.picture_labels.values.astype('U'))\n",
    "# pic_label_test = pic_label_count.transform(X_test.picture_labels.values.astype('U'))\n",
    "\n",
    "# picture_pca = PCA(n_components = 50)\n",
    "# picture_pca.fit(pic_label_train.toarray())\n",
    "# decomposed_pic_train = picture_pca.transform(pic_label_train.toarray())\n",
    "# decomposed_pic_test = picture_pca.transform(pic_label_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# description text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "description_vectorizer = TfidfVectorizer(sublinear_tf=True, \n",
    "                                         #stop_words='english', \n",
    "                                         ngram_range=(1, 2), \\\n",
    "                                         preprocessor = preprocessor)\n",
    "description_vectorizer.fit(X_train.description.values.astype('U'))\n",
    "tfidf_train = description_vectorizer.transform(X_train.description.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"The sparse document matrix has {} features\".format(tfidf_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dimension reduction: LSA\n",
    "tfidf_lsa = TruncatedSVD(n_components=150)\n",
    "reduced_tfidf_train = tfidf_lsa.fit_transform(tfidf_train)\n",
    "tfidf_test = description_vectorizer.transform(X_test.description.values.astype('U'))\n",
    "reduced_tfidf_test = tfidf_lsa.transform(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decomposed_X_train = X_train.drop('description', axis = 1)\n",
    "decomposed_X_test = X_test.drop('description', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_train = np.hstack((decomposed_X_train, reduced_tfidf_train))\n",
    "feature_test = np.hstack((decomposed_X_test, reduced_tfidf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a normalizer, then apply it to the features\n",
    "feature_scaler = MinMaxScaler()\n",
    "feature_scaler.fit(feature_train)\n",
    "feature_train = feature_scaler.transform(feature_train)\n",
    "feature_test = feature_scaler.transform(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=30, max_depth=None,min_samples_split=2, random_state=0)\n",
    "clf_RF = clf_RF.fit(feature_train, y_train)\n",
    "clf_RF.score(feature_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_ANN = MLPClassifier(activation='relu', hidden_layer_sizes=(165, ), solver='adam', alpha=1e-5, random_state=1)\n",
    "clf_ANN.fit(feature_train, y_train)\n",
    "clf_ANN.score(feature_test, y_test)\n",
    "#165 minus pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_AB = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=5), n_estimators=300, learning_rate=0.1)\n",
    "clf_AB = clf_AB.fit(feature_train, y_train)\n",
    "clf_AB.score(feature_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_GB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \\\n",
    "                                    max_depth=10, max_features = 'auto').fit(feature_train, y_train)\n",
    "clf_GB.score(feature_test, y_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (clf_GB.score(feature_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = clf_GB.predict(feature_test)\n",
    "print(metrics.classification_report(y_test, predicted, target_names=['Fake Seller', 'No Seller', 'Reseller']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#strip punctuation and numbers and extract stem of each feature \n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "    \n",
    "def preprocessor(text):\n",
    "    processed = re.sub(r'[#|\\!|\\-|\\+|:|//|\\']', \"\", text)\n",
    "    processed = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', ' ', processed).strip()\n",
    "    processed = re.sub('[\\s]+', ' ', processed).strip()\n",
    "    processed =  \" \".join([SnowballStemmer(\"english\").stem(word) for word in processed.split()])\n",
    "    return processed\n",
    "\n",
    "#K FOLD VALIDATION\n",
    "def extract_feature(train_idx, test_idx, feature_df):\n",
    "    X_train = feature_df.iloc[train_idx]\n",
    "    X_test = feature_df.iloc[test_idx]\n",
    "    \n",
    "    # tf-idf\n",
    "    description_vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2), preprocessor=preprocessor)\n",
    "    description_vectorizer.fit(X_train.description.values.astype('U'))\n",
    "    tfidf_train = description_vectorizer.transform(X_train.description.values.astype('U'))\n",
    "    tfidf_test = description_vectorizer.transform(X_test.description.values.astype('U'))\n",
    "\n",
    "    # LSA\n",
    "    tfidf_lsa = TruncatedSVD(n_components=150, random_state=0)\n",
    "    reduced_tfidf_train = tfidf_lsa.fit_transform(tfidf_train)\n",
    "    reduced_tfidf_test = tfidf_lsa.transform(tfidf_test)\n",
    "    \n",
    "    # combine text and non-text features\n",
    "    feature_train = np.hstack((X_train.drop('description', axis = 1), reduced_tfidf_train))\n",
    "    feature_test = np.hstack((X_test.drop('description', axis = 1), reduced_tfidf_test))\n",
    "    \n",
    "    # scale features\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    feature_scaler.fit(feature_train)\n",
    "    feature_train = feature_scaler.transform(feature_train)\n",
    "    feature_test = feature_scaler.transform(feature_test)\n",
    "    \n",
    "    print (feature_train.shape)\n",
    "    \n",
    "    return feature_train, feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "(28145, 164)\n",
      "Accuracy:0.8040358107147932\n",
      "Fold 2\n",
      "(28145, 164)\n",
      "Accuracy:0.7935199658945573\n",
      "Fold 3\n",
      "(28146, 164)\n",
      "Accuracy:0.7556850483229107\n",
      "Fold 4\n",
      "(28146, 164)\n",
      "Accuracy:0.7819783968163729\n",
      "Fold 5\n",
      "(28146, 164)\n",
      "Accuracy:0.7839681637293917\n",
      "Overall Accuracy: 0.7838374770956051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_RF = RandomForestClassifier(n_estimators=30, max_depth=None,min_samples_split=2, random_state=0)\n",
    "\n",
    "clf_AB = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=5), \\\n",
    "                            n_estimators=300, learning_rate=0.1, random_state=None)\n",
    "\n",
    "clf_GB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \\\n",
    "                                    max_depth=10, max_features = 'auto', random_state=0)\n",
    "clf_ANN = MLPClassifier(activation='relu', hidden_layer_sizes=(165, ), solver='adam', random_state=0)\n",
    "\n",
    "\n",
    "clf_model = clf_ANN\n",
    "model_result = []\n",
    "kf = KFold(n_splits=5, shuffle = True, random_state=0)\n",
    "i = 1\n",
    "for train, test in kf.split(features.values):\n",
    "    print ('Fold {}'.format(i))\n",
    "    train_final, test_final = extract_feature(train, test, features)\n",
    "    clf_model.fit(train_final, label[train])\n",
    "    output = clf_model.score(test_final, label[test])\n",
    "    model_result.append(output)\n",
    "    \n",
    "    print ('Accuracy:{}'.format(output))\n",
    "    i = i + 1\n",
    "\n",
    "print ('Overall Accuracy: {}'.format(np.mean(model_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ANN_resut = model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "(28145, 164)\n",
      "Accuracy:0.8497939462839278\n",
      "Fold 2\n",
      "(28145, 164)\n",
      "Accuracy:0.8438254938183886\n",
      "Fold 3\n",
      "(28146, 164)\n",
      "Accuracy:0.8439454235361\n",
      "Fold 4\n",
      "(28146, 164)\n",
      "Accuracy:0.8459351904491188\n",
      "Fold 5\n",
      "(28146, 164)\n",
      "Accuracy:0.8439454235361\n",
      "Overall Accuracy: 0.8454890955247271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_RF = RandomForestClassifier(n_estimators=30, max_depth=None,min_samples_split=2, random_state=0)\n",
    "\n",
    "clf_AB = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=5), \\\n",
    "                            n_estimators=300, learning_rate=0.1, random_state=None)\n",
    "\n",
    "clf_GB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \\\n",
    "                                    max_depth=10, max_features = 'auto', random_state=0)\n",
    "clf_ANN = MLPClassifier(activation='relu', hidden_layer_sizes=(165, ), solver='adam', random_state=0)\n",
    "\n",
    "\n",
    "clf_model = clf_RF\n",
    "model_result = []\n",
    "kf = KFold(n_splits=5, shuffle = True, random_state=0)\n",
    "i = 1\n",
    "for train, test in kf.split(features.values):\n",
    "    print ('Fold {}'.format(i))\n",
    "    train_final, test_final = extract_feature(train, test, features)\n",
    "    clf_model.fit(train_final, label[train])\n",
    "    output = clf_model.score(test_final, label[test])\n",
    "    model_result.append(output)\n",
    "    \n",
    "    print ('Accuracy:{}'.format(output))\n",
    "    i = i + 1\n",
    "\n",
    "print ('Overall Accuracy: {}'.format(np.mean(model_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_result = model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "(28145, 164)\n",
      "Accuracy:0.8489413102174223\n",
      "Fold 2\n",
      "(28145, 164)\n",
      "Accuracy:0.8415517976410403\n",
      "Fold 3\n",
      "(28146, 164)\n",
      "Accuracy:0.8381182490051166\n",
      "Fold 4\n",
      "(28146, 164)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_RF = RandomForestClassifier(n_estimators=30, max_depth=None,min_samples_split=2, random_state=0)\n",
    "\n",
    "clf_AB = AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth=5), \\\n",
    "                            n_estimators=300, learning_rate=0.1, random_state=None)\n",
    "\n",
    "clf_GB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \\\n",
    "                                    max_depth=6, max_features = 'auto', random_state=0)\n",
    "clf_ANN = MLPClassifier(activation='relu', hidden_layer_sizes=(165, ), solver='adam', random_state=0)\n",
    "\n",
    "\n",
    "clf_model = clf_GB\n",
    "model_result = []\n",
    "kf = KFold(n_splits=5, shuffle = True, random_state=0)\n",
    "i = 1\n",
    "for train, test in kf.split(features.values):\n",
    "    print ('Fold {}'.format(i))\n",
    "    train_final, test_final = extract_feature(train, test, features)\n",
    "    # cProfile.run('clf_model.fit(train_final, label[train])')\n",
    "    clf_model.fit(train_final, label[train])\n",
    "    output = clf_model.score(test_final, label[test])\n",
    "    model_result.append(output)\n",
    "    \n",
    "    print ('Accuracy:{}'.format(output))\n",
    "    i = i + 1\n",
    "\n",
    "print ('Overall Accuracy: {}'.format(np.mean(model_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "i = 1\n",
    "worst_train = []\n",
    "worst_test = []\n",
    "for train, test in kf.split(features.values):\n",
    "    if i == 2:\n",
    "        worst_train =train \n",
    "        worst_test = test \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_reult = [0.69546681824641188, 0.66576666192979961,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ANN_result = [0.69546681824641188, 0.75444081284638342, ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
